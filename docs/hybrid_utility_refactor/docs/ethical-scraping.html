<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Ethical Scraping Implementation</title>
    <link rel="stylesheet" href="../css/design-system.css" />
    <link rel="stylesheet" href="../css/main.css" />
  </head>

  <body>
    <!-- Header -->
    <div id="header-placeholder"></div>

    <div class="doc-container">
      <div class="container">
        <div class="doc-content">
          <div class="nav-breadcrumb">
            <a href="index.html">Documentation Home</a>
          </div>
          <h1>Ethical Scraping Implementation</h1>

          <h4>Status:</h4>
          Implemented
          <h4>Version:</h4>
          2.0
          <h4>Last Updated:</h4>
          October 8, 2025

          <h2>Overview</h2>

          <p>
            Capcat implements comprehensive ethical scraping practices to ensure
            respectful and compliant content collection from news sources.
          </p>

          <h2>Core Principles</h2>

          <h3>1. Prefer Official APIs &gt; RSS &gt; HTML Scraping</h3>

          <h4>Implementation Priority:</h4>
          <ol>
            <li>
              <h4>Official APIs</h4>
              - Always preferred when available
            </li>
            <li>
              <h4>RSS/Atom Feeds</h4>
              - Second choice, widely supported
            </li>
            <li>
              <h4>HTML Scraping</h4>
              - Last resort, only when no alternatives exist
            </li>
          </ol>

          <h4>Current Source Distribution:</h4>
          <ul>
            <li>
              Custom sources (10): Use RSS feeds or specialized approaches
            </li>
            <li>Config-driven sources (1): InfoQ uses RSS</li>
          </ul>

          <h3>2. Robots.txt Compliance</h3>

          <h4>Implementation:</h4>
          <code>core/ethical_scraping.py</code>

          <h4>Features:</h4>
          <ul>
            <li>Automatic robots.txt fetching and parsing</li>
            <li>15-minute TTL cache to reduce server load</li>
            <li>Crawl-delay extraction and enforcement</li>
            <li>Path validation against disallow rules</li>
          </ul>

          <h4>Cache Management:</h4>
          <pre><code>from core.ethical_scraping import get_ethical_manager

manager = get_ethical_manager()
parser, crawl_delay = manager.get_robots_txt(base_url)
allowed, reason = manager.can_fetch(url)
</code></pre>

          <h4>Cache Statistics:</h4>
          <ul>
            <li>TTL: 15 minutes</li>
            <li>Auto-cleanup of stale entries</li>
            <li>Per-domain caching</li>
          </ul>

          <h3>3. User-Agent Identification</h3>

          <h4>Standard User-Agent:</h4>
          <pre><code>Capcat/2.0 (Personal news archiver)
</code></pre>

          <h4>Implementation Locations:</h4>
          <ul>
            <li><code>core/config.py:35</code> - Global default</li>
            <li>
              <code>core/source_system/source_config.py:54</code> - Source
              config default
            </li>
            <li>
              <code>core/source_system/config_driven_source.py:119</code> - RSS
              requests
            </li>
            <li>
              <code>sources/active/custom/lb/source.py:61,460</code> - Lobsters
              custom headers
            </li>
          </ul>

          <h4>Format Guidelines:</h4>
          <ul>
            <li>Product name and version: <code>Capcat/2.0</code></li>
            <li>Purpose description: <code>(Personal news archiver)</code></li>
            <li>No personal information or URLs</li>
          </ul>

          <h3>4. Rate Limiting</h3>

          <h4>Enforcement:</h4>
          <ul>
            <li>Minimum 1 request per second globally</li>
            <li>Respect robots.txt crawl-delay directives</li>
            <li>Per-domain rate limiting tracking</li>
          </ul>

          <h4>Current Rate Limits:</h4>
          <ul>
            <li>
              <h4>InfoQ</h4>
              → <code>20.0s</code> → 20.0s → Compliant
            </li>
            <li>
              <h4>BBC</h4>
              → Custom → N/A → Compliant
            </li>
            <li>
              <h4>HN</h4>
              → API → N/A → Compliant
            </li>
            <li>
              <h4>Lobsters</h4>
              → RSS → N/A → Compliant
            </li>
            <li>
              <h4>Others</h4>
              → 1.0-3.0s → Varies → Compliant
            </li>
          </ul>

          <h3>5. Error Handling with Exponential Backoff</h3>

          <h4>Implementation:</h4>
          <code>core/ethical_scraping.py:request_with_backoff()</code>

          <h4>HTTP Status Codes Handled:</h4>
          <ul>
            <li>
              <h4>429 (Too Many Requests):</h4>
            </li>
            <li>Respect <code>Retry-After</code> header</li>
            <li>Exponential backoff if header missing</li>
            <li>Initial delay: 1.0s, multiplier: 2x</li>
          </ul>

          <ul>
            <li>
              <h4>503 (Service Unavailable):</h4>
            </li>
            <li>Exponential backoff</li>
            <li>Initial delay: 1.0s, multiplier: 2x</li>
            <li>Max retries: 3 attempts</li>
          </ul>

          <h4>Backoff Strategy:</h4>
          <pre><code>Attempt 1: 1.0s delay
Attempt 2: 2.0s delay
Attempt 3: 4.0s delay
</code></pre>

          <h2>Implementation Details</h2>

          <h3>EthicalScrapingManager Class</h3>

          <h4>Location:</h4>
          <code>core/ethical_scraping.py</code>

          <h4>Key Methods:</h4>

          <pre><code>class EthicalScrapingManager:
    def __init__(self, user_agent: str = &quot;Capcat/2.0&quot;)

    def get_robots_txt(self, base_url: str, timeout: int = 10) -&gt; Tuple[RobotFileParser, float]

    def can_fetch(self, url: str) -&gt; Tuple[bool, str]

    def enforce_rate_limit(self, domain: str, crawl_delay: float, min_delay: float = 1.0)

    def request_with_backoff(
        self, session: requests.Session, url: str,
        method: str = &quot;GET&quot;, max_retries: int = 3,
        initial_delay: float = 1.0, **kwargs
    ) -&gt; requests.Response

    def validate_source_config(self, base_url: str, rate_limit: float) -&gt; Tuple[bool, str]

    def get_cache_stats(self) -&gt; Dict[str, any]

    def clear_stale_cache(self)
</code></pre>

          <h3>Usage Example</h3>

          <pre><code>from core.ethical_scraping import get_ethical_manager

# Get global manager instance
manager = get_ethical_manager()

# Validate source configuration
is_valid, message = manager.validate_source_config(
    base_url=&quot;https://example.com/news/&quot;,
    rate_limit=2.0
)

if not is_valid:
    print(f&quot;Configuration issue: {message}&quot;)

# Make ethical request with backoff
response = manager.request_with_backoff(
    session=requests.Session(),
    url=&quot;https://example.com/article&quot;,
    timeout=30
)
</code></pre>

          <h2>Compliance Audit Process</h2>

          <h4>Tool:</h4>
          <code>audit_ethical_compliance.py</code> (temporary, created
          on-demand)

          <h4>Audit Checks:</h4>
          <ol>
            <li>Robots.txt fetching and parsing</li>
            <li>Crawl-delay requirement extraction</li>
            <li>Path allowance validation</li>
            <li>RSS feed availability detection</li>
            <li>Rate limit compliance verification</li>
            <li>Bundle membership verification</li>
          </ol>

          <h4>Last Audit:</h4>
          October 8, 2025
          <h4>Result:</h4>
          All active sources compliant

          <h4>Findings:</h4>
          <ul>
            <li>1 active config-driven source (InfoQ)</li>
            <li>5 orphaned sources moved to inactive</li>
            <li>0 active violations</li>
          </ul>

          <h2>Source Compliance Status</h2>

          <h3>Active Sources (11)</h3>

          <h4>Config-Driven (1):</h4>
          <ul>
            <li>InfoQ - RSS feed, 20.0s rate limit, compliant</li>
          </ul>

          <h4>Custom Sources (10):</h4>
          <ul>
            <li>Hacker News - Official API</li>
            <li>Lobsters - RSS feed</li>
            <li>BBC - Custom implementation</li>
            <li>Gizmodo - RSS feed</li>
            <li>Futurism - RSS feed</li>
            <li>IEEE Spectrum - RSS feed</li>
            <li>Nature - RSS feed</li>
            <li>Scientific American - RSS feed</li>
            <li>LessWrong - GraphQL API</li>
            <li>MIT Tech Review - RSS feed (inactive in bundles)</li>
          </ul>

          <h2>Red Flags to Avoid</h2>

          <h3>Never Do:</h3>
          <ul>
            <li>Ignore robots.txt directives</li>
            <li>Bypass anti-bot protection</li>
            <li>Scrape paths explicitly blocked</li>
            <li>Use aggressive rate limits (&lt;1s without permission)</li>
            <li>Impersonate browser User-Agents deceptively</li>
            <li>Scrape authentication-required content</li>
            <li>Access paywalled content</li>
            <li>Ignore 429/503 error responses</li>
          </ul>

          <h3>Always Do:</h3>
          <ul>
            <li>Check robots.txt before scraping</li>
            <li>Respect crawl-delay directives</li>
            <li>Use RSS/API when available</li>
            <li>Identify as &quot;Capcat/2.0 (Personal news archiver)&quot;</li>
            <li>Handle errors gracefully with backoff</li>
            <li>Cache robots.txt to reduce load</li>
            <li>Rate limit: minimum 1 req/s</li>
            <li>Document scraping methodology</li>
          </ul>

          <h2>Configuration Reference</h2>

          <h3>Rate Limit Configuration</h3>

          <h4>File:</h4>
          <code>sources/active/config_driven/configs/iq.yaml</code>

          <pre><code># Request configuration
timeout: 15
rate_limit: 20.0  # Must be &gt;= robots.txt crawl-delay
</code></pre>

          <h3>User-Agent Configuration</h3>

          <h4>File:</h4>
          <code>core/config.py</code>

          <pre><code>@dataclass
class NetworkConfig:
    user_agent: str = &quot;Capcat/2.0 (Personal news archiver)&quot;
</code></pre>

          <h3>RSS Discovery Configuration</h3>

          <h4>File:</h4>
          <code>sources/active/config_driven/configs/iq.yaml</code>

          <pre><code># Discovery method - use RSS for latest news articles only
discovery:
  method: &quot;rss&quot;
  rss_url: &quot;https://feed.infoq.com&quot;
  max_articles: 30
</code></pre>

          <h2>Testing Compliance</h2>

          <h3>Manual Testing</h3>

          <pre><code># Test InfoQ RSS implementation
./capcat fetch iq --count 5

# Test with verbose logging
./capcat -L compliance.log fetch iq --count 5

# View logs
tail -f compliance.log
</code></pre>

          <h3>Automated Validation</h3>

          <pre><code>from core.ethical_scraping import get_ethical_manager

manager = get_ethical_manager()

# Validate all sources
sources = [&quot;iq&quot;, &quot;hn&quot;, &quot;lb&quot;, &quot;bbc&quot;]
for source_id in sources:
    config = load_source_config(source_id)
    is_valid, message = manager.validate_source_config(
        config.base_url,
        config.rate_limit
    )
    print(f&quot;{source_id}: {message}&quot;)
</code></pre>

          <h2>References</h2>

          <h3>Documentation</h3>
          <ul>
            <li>
              <a href="https://www.robotstxt.org/">Robots.txt Specification</a>
            </li>
            <li>
              <a
                href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429"
                >HTTP 429 Too Many Requests</a
              >
            </li>
            <li>
              <a href="https://www.rssboard.org/rss-specification"
                >RSS 2.0 Specification</a
              >
            </li>
            <li>
              <a href="https://datatracker.ietf.org/doc/html/rfc4287"
                >Atom Syndication Format</a
              >
            </li>
          </ul>

          <h3>Internal Documentation</h3>
          <ul>
            <li>
              <code>ETHICAL_COMPLIANCE_REPORT.md</code> - October 2025 audit
              results
            </li>
            <li><code>docs/architecture.md</code> - System architecture</li>
            <li>
              <code>docs/source-development.md</code> - Adding new sources
            </li>
          </ul>

          <h2>Maintenance</h2>

          <h3>Regular Tasks</h3>

          <h4>Monthly:</h4>
          <ul>
            <li>Review rate limits for new sources</li>
            <li>Check for RSS feed availability</li>
            <li>Update User-Agent if needed</li>
          </ul>

          <h4>Quarterly:</h4>
          <ul>
            <li>Run full compliance audit</li>
            <li>Review and update robots.txt cache</li>
            <li>Validate all active sources</li>
          </ul>

          <h4>As Needed:</h4>
          <ul>
            <li>Update when adding new sources</li>
            <li>Respond to robots.txt changes</li>
            <li>Handle 429/503 rate limit errors</li>
          </ul>

          <h3>Contact</h3>

          <p>For questions about ethical scraping implementation:</p>
          <ol>
            <li>Review this documentation</li>
            <li>Check <code>ETHICAL_COMPLIANCE_REPORT.md</code></li>
            <li>
              Review source-specific configs in <code>sources/active/</code>
            </li>
          </ol>

          <script type="module">
            import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs";
            mermaid.initialize({
              startOnLoad: true,
              theme: "base",
              themeVariables: {
                // Primary colors from design system (Capcat orange palette)
                primaryColor: "#FFD4B7", // --orange-200
                primaryTextColor: "#201419", // --ink
                primaryBorderColor: "#F1540E", // --orange-500 / --accent-primary

                // Line and edge colors
                lineColor: "#58444c", // --ink-medium

                // Secondary colors
                secondaryColor: "#FFEADB", // --orange-100
                tertiaryColor: "#f9f8ed", // --accent-cream-primary

                // Text colors
                textColor: "#201419", // --ink
                mainBkg: "#FAF8EE", // --cream

                // Node styling
                nodeBorder: "#F1540E", // --accent-primary
                clusterBkg: "#faf2e7", // --accent-cream-light
                clusterBorder: "#D44400", // --orange-600 / --accent-hover

                // Font
                fontFamily:
                  '-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif',
                fontSize: "16px",
              },
              flowchart: {
                nodeSpacing: 50,
                rankSpacing: 50,
                padding: 15,
                useMaxWidth: true,
                htmlLabels: true,
                curve: "basis",
              },
            });

            // Add copy buttons to Mermaid diagrams after rendering
            document.addEventListener("DOMContentLoaded", function () {
              const mermaidDivs = document.querySelectorAll(".mermaid");

              mermaidDivs.forEach(function (mermaidDiv) {
                // Get the original Mermaid source code
                const mermaidSource = mermaidDiv.textContent;

                // Create container wrapper
                const container = document.createElement("div");
                container.className = "mermaid-container";

                // Create copy button
                const copyBtn = document.createElement("button");
                copyBtn.className = "mermaid-copy-btn";
                copyBtn.textContent = "Copy Mermaid Code";
                copyBtn.setAttribute(
                  "title",
                  "Copy diagram code for Draw.io, Mermaid Live, etc.",
                );

                copyBtn.addEventListener("click", function () {
                  navigator.clipboard
                    .writeText(mermaidSource)
                    .then(function () {
                      copyBtn.textContent = "Copied!";
                      copyBtn.classList.add("copied");

                      setTimeout(function () {
                        copyBtn.textContent = "Copy Mermaid Code";
                        copyBtn.classList.remove("copied");
                      }, 2000);
                    });
                });

                // Wrap the mermaid div in container and add button
                mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
                container.appendChild(mermaidDiv);
                container.appendChild(copyBtn);
              });
            });
          </script>
          <script src="../js/main.js"></script>

          <nav class="chapter-navigation" aria-label="Chapter navigation">
            <div class="next-chapter-content">
              <span class="next-chapter-label">Next Chapter</span>
              <span class="next-chapter-arrow" aria-hidden="true">→</span>
              <a href="dependencies.html" class="next-chapter-link" rel="next"
                >Dependency Management</a
              >
            </div>
          </nav>
        </div>
      </div>
    </div>

    <!-- Footer -->
    <div id="footer-placeholder"></div>

    <!-- Back to Top Button -->
    <button id="backToTopBtn" title="Go to top">
      <svg
        xmlns="http://www.w3.org/2000/svg"
        height="24"
        viewBox="0 0 24 24"
        width="24"
      >
        <path d="M0 0h24v24H0V0z" fill="none" />
        <path
          fill="currentColor"
          d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6 1.41 1.41z"
        />
      </svg>
    </button>
      <script src="../../js/includes-loader.js"></script>
  </body>
</html>
