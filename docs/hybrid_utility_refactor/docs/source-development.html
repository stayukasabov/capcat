---
---
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Source Development Guide</title>
    <link rel="stylesheet" href="../css/design-system.css">
    <link rel="stylesheet" href="../css/main.css">
</head>
<body>
    <!-- Header -->
    {% include header.html %}


    <div class="doc-container">
        <div class="container">
            <div class="doc-content">

<div class="nav-breadcrumb"><a href="index.html">Documentation Home</a></div>
<h1>Source Development Guide</h1>

<p>Complete guide for developing new sources in the Capcat hybrid architecture. Choose between config-driven (simple) or custom implementation (complex) based on your requirements.</p>

<h2>Decision Matrix: Config-Driven vs Custom</h2>

<ul>
<li><h4>
<h4>Development Time</h4></h4> → 15-30 minutes</li>
<li><h4>
<h4>Coding Required</h4></h4> → None (YAML only)</li>
<li><h4>
<h4>Flexibility</h4></h4> → Limited to standard patterns</li>
<li><h4>
<h4>Best For</h4></h4> → Standard news sites</li>
<li><h4>
<h4>Maintenance</h4></h4> → Configuration updates</li>
<li><h4>
<h4>Examples</h4></h4> → InfoQ, Euronews, Straits Times</li>
</ul>

<h2>RSS-First Development Rule</h2>

<h4>MANDATORY</h4> When adding sources, check RSS feeds first and use the links to access content. RSS-based extraction often provides cleaner, more reliable content than HTML scraping, especially for React/SPA websites.

<h4>RSS-First Benefits</h4>
<ul>
<li>Bypasses bot protection and JavaScript rendering issues</li>
<li>Provides clean, structured content</li>
<li>Often includes full article text in descriptions</li>
<li>More reliable than HTML selectors that frequently change</li>
</ul>

<h4>Implementation</h4> Configure <code>rss_config</code> in source YAML with <code>use_rss_content: true</code> to extract content directly from RSS descriptions.

<h2>Config-Driven Sources</h2>

<h4>Use When</h4> Standard news website with straightforward article listing and content structure, or RSS-based content extraction.

<h3>1. Create Configuration File</h3>

<pre><code># sources/active/config_driven/configs/newsource.yaml
display_name: &quot;News Source&quot;
base_url: &quot;https://newsource.com/&quot;
category: &quot;general&quot;  # tech, science, business, general
timeout: 10.0
rate_limit: 1.0

# RSS-based content extraction (preferred method)
rss_config:
  feed_url: &quot;https://newsource.com/feed.xml&quot;
  use_rss_content: true
  content_field: &quot;description&quot;  # Extract content from RSS description field

# Required: Article discovery selectors
article_selectors:
  - &quot;.headline a&quot;
  - &quot;.article-title a&quot;
  - &quot;h2.title a&quot;

# Required: Content extraction selectors
content_selectors:
  - &quot;.article-content&quot;
  - &quot;.post-body&quot;
  - &quot;div.content&quot;

# Optional: Skip patterns (URLs to ignore)
skip_patterns:
  - &quot;/about&quot;
  - &quot;/contact&quot;
  - &quot;/advertising&quot;
  - &quot;?utm_&quot;

# Optional: Comment support
supports_comments: false

# Image processing configuration
image_processing:
  selectors:
    - &quot;img&quot;
    - &quot;.content img&quot;
    - &quot;article img&quot;

  url_patterns:
    - &quot;newsource.com/&quot;
    - &quot;cdn.newsource.com/&quot;
    - &quot;images.newsource.com/&quot;

  # Allow URLs without traditional extensions for modern CDNs
  allow_extensionless: true

  skip_selectors:
    - &quot;.sidebar img&quot;
    - &quot;.navigation img&quot;
    - &quot;.header img&quot;
    - &quot;.avatar img&quot;

# Optional: Additional configuration
custom_config:
  user_agent: &quot;Custom User Agent&quot;
  headers:
    Accept: &quot;text/html,application/xhtml+xml&quot;
</code></pre>

<h3>2. Test Configuration</h3>
<pre><code># Test source discovery
python -c &quot;from core.source_system.source_registry import get_source_registry; print(&#x27;newsource&#x27; in get_source_registry().get_available_sources())&quot;

# Test source functionality
./capcat fetch newsource --count 3
</code></pre>

<h3>3. Validation</h3>
<pre><code># Run validation
python -c &quot;
from core.source_system.source_registry import get_source_registry
registry = get_source_registry()
errors = registry.validate_all_sources(deep_validation=True)
print(f&#x27;newsource errors: {errors.get(\&quot;newsource\&quot;, [])}&#x27;)
&quot;
</code></pre>

<h2>Template System Integration</h2>

<h4>Universal HTML Generation</h4> All sources can leverage the template system for consistent navigation and professional output.

<h3>Adding Template Support to Sources</h3>

<pre><code># Add to your source config.yaml (both config-driven and custom)
template:
  variant: &quot;article-with-comments&quot;  # or &quot;article-no-comments&quot;
  navigation:
    back_to_news_url: &quot;../../news.html&quot;
    back_to_news_text: &quot;Back to News&quot;
    has_comments: true              # false for news sources without comments
    comments_url: &quot;comments.html&quot;   # only if has_comments: true
    comments_text: &quot;View Comments&quot;  # only if has_comments: true
</code></pre>

<h3>Template Variants</h3>

<ul>
<li><h4><code>article-with-comments</code></h4> For sources like HN, Lobsters, LessWrong with comment systems</li>
<li><h4><code>article-no-comments</code></h4> For news sources like BBC, CNN, Nature without comments</li>
<li><h4><code>comments-with-navigation</code></h4> Automatically used for all comments pages</li>
</ul>

<h3>Benefits</h3>

<ul>
<li><h4>Automatic HTML Generation</h4> Professional navigation without custom HTML code</li>
<li><h4>Consistent Experience</h4> Same navigation patterns across all sources</li>
<li><h4>Conditional Comments</h4> Comments links only shown when comments exist</li>
<li><h4>Responsive Design</h4> Mobile-friendly with dark/light theme support</li>
</ul>

<h3>Integration</h3>

<h2>UTF-8 Encoding Handling</h2>

<h4>Native UTF-8 Support</h4> Capcat uses Python&#x27;s built-in UTF-8 handling and BeautifulSoup&#x27;s automatic encoding detection for reliable character processing.

<h3>Encoding Best Practices</h3>

<ul>
<li>All content is processed using proper UTF-8 encoding</li>
<li>BeautifulSoup automatically detects and handles various character encodings</li>
<li>No additional text processing needed - modern websites use proper UTF-8</li>
<li>Special characters (é, ñ, ö, etc.) are preserved correctly</li>
</ul>

<h2>Custom Sources</h2>

<h4>Use When</h4> Complex scraping logic, API integration, comment systems, or anti-bot protection handling required.

<h3>1. Create Source Structure</h3>
<pre><code>mkdir -p sources/active/custom/newsource
touch sources/active/custom/newsource/source.py
touch sources/active/custom/newsource/config.yaml
</code></pre>

<h3>2. Basic Configuration</h3>
<pre><code># sources/active/custom/newsource/config.yaml
display_name: &quot;News Source&quot;
base_url: &quot;https://newsource.com/&quot;
category: &quot;general&quot;
timeout: 10.0
rate_limit: 1.0
supports_comments: true  # If implementing comment system
</code></pre>

<h3>3. Source Implementation</h3>
<pre><code># sources/active/custom/newsource/source.py
from typing import List, Dict, Optional
from core.source_system.base_source import BaseSource, SourceConfig
from core.logging_config import get_logger

class NewsSourceSource(BaseSource):
    &quot;&quot;&quot;Custom implementation for News Source.&quot;&quot;&quot;

    def __init__(self, config: SourceConfig, session=None):
        super().__init__(config, session)
        self.logger = get_logger(__name__)

    def get_articles(self, count: int = 30) -&gt; List[Dict]:
        &quot;&quot;&quot;
        Get articles from the source.

        Args:
            count: Number of articles to fetch

        Returns:
            List of article dictionaries with keys: title, url, summary
        &quot;&quot;&quot;
        try:
            self.logger.info(f&quot;Fetching {count} articles from {self.config.display_name}&quot;)

            # Step 1: Get main page
            response = self.session.get(
                self.config.base_url,
                timeout=self.config.timeout,
                headers=self._get_headers()
            )
            response.raise_for_status()

            # Step 2: Parse articles
            soup = self._get_soup(response.text)
            articles = []

            # Custom parsing logic
            for article_elem in soup.select(&#x27;.article-item&#x27;):
                title_elem = article_elem.select_one(&#x27;.title a&#x27;)
                summary_elem = article_elem.select_one(&#x27;.summary&#x27;)

                if title_elem and title_elem.get(&#x27;href&#x27;):
                    article = {
                        &#x27;title&#x27;: title_elem.get_text(strip=True),
                        &#x27;url&#x27;: self._resolve_url(title_elem[&#x27;href&#x27;]),
                        &#x27;summary&#x27;: summary_elem.get_text(strip=True) if summary_elem else &#x27;&#x27;
                    }
                    articles.append(article)

                    if len(articles) &gt;= count:
                        break

            self.logger.info(f&quot;Successfully fetched {len(articles)} articles&quot;)
            return articles

        except Exception as e:
            self.logger.error(f&quot;Error fetching articles: {e}&quot;)
            return []

    def get_article_content(self, url: str) -&gt; Optional[str]:
        &quot;&quot;&quot;
        Get full content for a specific article.

        Args:
            url: Article URL

        Returns:
            Article content as HTML string
        &quot;&quot;&quot;
        try:
            response = self.session.get(url, timeout=self.config.timeout)
            response.raise_for_status()

            soup = self._get_soup(response.text)

            # Try multiple content selectors
            for selector in [&#x27;.article-content&#x27;, &#x27;.post-body&#x27;, &#x27;div.content&#x27;]:
                content_elem = soup.select_one(selector)
                if content_elem:
                    return str(content_elem)

            self.logger.warning(f&quot;No content found for {url}&quot;)
            return None

        except Exception as e:
            self.logger.error(f&quot;Error fetching content for {url}: {e}&quot;)
            return None

    def get_comments(self, url: str) -&gt; List[Dict]:
        &quot;&quot;&quot;
        Get comments for an article (if supported).

        Args:
            url: Article URL

        Returns:
            List of comment dictionaries
        &quot;&quot;&quot;
        if not self.config.supports_comments:
            return []

        try:
            response = self.session.get(url, timeout=self.config.timeout)
            response.raise_for_status()

            soup = self._get_soup(response.text)
            comments = []

            # Custom comment parsing logic
            for comment_elem in soup.select(&#x27;.comment&#x27;):
                author_elem = comment_elem.select_one(&#x27;.author&#x27;)
                text_elem = comment_elem.select_one(&#x27;.comment-text&#x27;)

                if author_elem and text_elem:
                    comment = {
                        &#x27;author&#x27;: author_elem.get_text(strip=True),
                        &#x27;text&#x27;: text_elem.get_text(strip=True),
                        &#x27;timestamp&#x27;: self._extract_timestamp(comment_elem)
                    }
                    comments.append(comment)

            return comments

        except Exception as e:
            self.logger.error(f&quot;Error fetching comments for {url}: {e}&quot;)
            return []

    def validate_config(self) -&gt; List[str]:
        &quot;&quot;&quot;Validate source-specific configuration.&quot;&quot;&quot;
        errors = []

        # Add custom validation logic
        if not self.config.base_url.startswith(&#x27;https://&#x27;):
            errors.append(&quot;base_url must use HTTPS&quot;)

        return errors

    def _get_headers(self) -&gt; Dict[str, str]:
        &quot;&quot;&quot;Get custom headers for requests.&quot;&quot;&quot;
        headers = {
            &#x27;User-Agent&#x27;: &#x27;Mozilla/5.0 (compatible; Capcat/2.0)&#x27;,
            &#x27;Accept&#x27;: &#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;,
            &#x27;Accept-Language&#x27;: &#x27;en-US,en;q=0.5&#x27;,
            &#x27;Accept-Encoding&#x27;: &#x27;gzip, deflate&#x27;,
            &#x27;Connection&#x27;: &#x27;keep-alive&#x27;,
        }

        # Add custom headers from config
        custom_headers = self.config.custom_config.get(&#x27;headers&#x27;, {})
        headers.update(custom_headers)

        return headers

    def _extract_timestamp(self, element) -&gt; Optional[str]:
        &quot;&quot;&quot;Extract timestamp from comment element.&quot;&quot;&quot;
        # Custom timestamp extraction logic
        time_elem = element.select_one(&#x27;.timestamp, .date, time&#x27;)
        if time_elem:
            return time_elem.get(&#x27;datetime&#x27;) or time_elem.get_text(strip=True)
        return None
</code></pre>

<h3>4. Advanced Custom Features</h3>

<h3>API Integration Example</h3>
<pre><code>def get_articles(self, count: int = 30) -&gt; List[Dict]:
    &quot;&quot;&quot;API-based article fetching.&quot;&quot;&quot;
    api_url = f&quot;{self.config.base_url}/api/articles&quot;

    response = self.session.get(
        api_url,
        params={&#x27;limit&#x27;: count, &#x27;format&#x27;: &#x27;json&#x27;},
        headers=self._get_api_headers()
    )

    data = response.json()
    return [
        {
            &#x27;title&#x27;: item[&#x27;title&#x27;],
            &#x27;url&#x27;: item[&#x27;permalink&#x27;],
            &#x27;summary&#x27;: item.get(&#x27;excerpt&#x27;, &#x27;&#x27;)
        }
        for item in data.get(&#x27;articles&#x27;, [])
    ]

def _get_api_headers(self) -&gt; Dict[str, str]:
    &quot;&quot;&quot;API-specific headers.&quot;&quot;&quot;
    return {
        &#x27;Accept&#x27;: &#x27;application/json&#x27;,
        &#x27;User-Agent&#x27;: &#x27;Capcat/2.0 API Client&#x27;
    }
</code></pre>

<h3>Anti-Bot Protection Handling</h3>
<pre><code>def _handle_anti_bot_protection(self, response):
    &quot;&quot;&quot;Handle CloudFlare or similar protection.&quot;&quot;&quot;
    if &#x27;cloudflare&#x27; in response.text.lower():
        self.logger.warning(&quot;CloudFlare protection detected&quot;)
        # Implement CloudFlare bypass logic
        # Or use alternative endpoints

    return response
</code></pre>

<h3>Dynamic Content Loading</h3>
<pre><code>def _wait_for_dynamic_content(self, soup):
    &quot;&quot;&quot;Handle JavaScript-loaded content.&quot;&quot;&quot;
    # Check for loading indicators
    if soup.select(&#x27;.loading, .spinner&#x27;):
        time.sleep(2)  # Wait for content to load
        # Re-fetch or use selenium for complex cases
</code></pre>

<h2>Testing Your Source</h2>

<h3>1. Basic Functionality Test</h3>
<pre><code># test_newsource.py
import unittest
from core.source_system.source_registry import get_source_registry

class TestNewsSource(unittest.TestCase):
    def setUp(self):
        self.registry = get_source_registry()
        self.source = self.registry.get_source(&#x27;newsource&#x27;)

    def test_get_articles(self):
        articles = self.source.get_articles(count=5)
        self.assertGreater(len(articles), 0)
        self.assertIn(&#x27;title&#x27;, articles[0])
        self.assertIn(&#x27;url&#x27;, articles[0])

    def test_get_content(self):
        articles = self.source.get_articles(count=1)
        if articles:
            content = self.source.get_article_content(articles[0][&#x27;url&#x27;])
            self.assertIsNotNone(content)

if __name__ == &#x27;__main__&#x27;:
    unittest.main()
</code></pre>

<h3>2. Integration Test</h3>
<pre><code># Test with actual command
./capcat fetch newsource --count 3

# Verify output structure
ls &quot;../News/news_$(date +%d-%m-%Y)/NewsSource_$(date +%d-%m-%Y)/&quot;
</code></pre>

<h3>3. Performance Test</h3>
<pre><code>import time
from core.source_system.performance_monitor import PerformanceMonitor

monitor = PerformanceMonitor()
start_time = time.time()

source = registry.get_source(&#x27;newsource&#x27;)
articles = source.get_articles(count=10)

duration = time.time() - start_time
print(f&quot;Fetched {len(articles)} articles in {duration:.2f} seconds&quot;)
</code></pre>

<h2>Best Practices</h2>

<h3>Code Quality</h3>
<ul>
<li><h4>Follow PEP 8</h4> Use flake8 for linting</li>
<li><h4>Type Hints</h4> Include type annotations</li>
<li><h4>Documentation</h4> Google-style docstrings</li>
<li><h4>Error Handling</h4> Comprehensive exception management</li>
<li><h4>Logging</h4> Use structured logging</li>
</ul>

<h3>Performance</h3>
<ul>
<li><h4>Reuse Sessions</h4> Use provided session instance</li>
<li><h4>Rate Limiting</h4> Respect site rate limits</li>
<li><h4>Caching</h4> Cache expensive operations</li>
<li><h4>Timeouts</h4> Always set request timeouts</li>
</ul>

<h3>Security</h3>
<ul>
<li><h4>User Agents</h4> Use realistic user agent strings</li>
<li><h4>Headers</h4> Include standard browser headers</li>
<li><h4>Respect robots.txt</h4> Check site crawling policies</li>
<li><h4>Rate Limiting</h4> Avoid overwhelming servers</li>
</ul>

<h3>Maintainability</h3>
<ul>
<li><h4>Configuration</h4> Use config for all site-specific values</li>
<li><h4>Selectors</h4> Make CSS selectors configurable</li>
<li><h4>Validation</h4> Implement thorough config validation</li>
<li><h4>Testing</h4> Create comprehensive test suites</li>
</ul>

<h2>Debugging</h2>

<h3>Enable Debug Logging</h3>
<pre><code>import logging
logging.getLogger(&#x27;core.source_system&#x27;).setLevel(logging.DEBUG)
</code></pre>

<h3>Test Selectors</h3>
<pre><code>from bs4 import BeautifulSoup
import requests

# Test selectors manually
response = requests.get(&#x27;https://newsource.com/&#x27;)
soup = BeautifulSoup(response.text, &#x27;html.parser&#x27;)

# Test article selectors
articles = soup.select(&#x27;.headline a&#x27;)
print(f&quot;Found {len(articles)} articles&quot;)

# Test content selectors
for selector in [&#x27;.article-content&#x27;, &#x27;.post-body&#x27;]:
    elements = soup.select(selector)
    print(f&quot;Selector &#x27;{selector}&#x27;: {len(elements)} elements&quot;)
</code></pre>

<h3>Performance Debugging</h3>
<pre><code>import time
from core.source_system.performance_monitor import get_performance_monitor

monitor = get_performance_monitor()

# Check source metrics
metrics = monitor.get_source_metrics(&#x27;newsource&#x27;)
print(f&quot;Success rate: {metrics.success_rate:.1f}%&quot;)
print(f&quot;Avg response time: {metrics.avg_response_time:.2f}s&quot;)
</code></pre>

<h2>Checklist</h2>

<h3>Config-Driven Source Checklist</h3>
<ul>
<li>[ ] YAML configuration created</li>
<li>[ ] Article selectors defined and tested</li>
<li>[ ] Content selectors defined and tested</li>
<li>[ ] Skip patterns configured (if needed)</li>
<li>[ ] Source discoverable by registry</li>
<li>[ ] Basic fetch test successful</li>
<li>[ ] Validation passes</li>
</ul>

<h3>Custom Source Checklist</h3>
<ul>
<li>[ ] Source directory structure created</li>
<li>[ ] BaseSource subclass implemented</li>
<li>[ ] get_articles() method implemented</li>
<li>[ ] get_article_content() method implemented</li>
<li>[ ] get_comments() method implemented (if applicable)</li>
<li>[ ] validate_config() method implemented</li>
<li>[ ] Error handling comprehensive</li>
<li>[ ] Logging implemented</li>
<li>[ ] Unit tests created</li>
<li>[ ] Integration test successful</li>
<li>[ ] Performance acceptable</li>
</ul>



<em>Following this guide ensures your source integrates seamlessly with the Capcat hybrid architecture while maintaining high quality and performance standards.</em>

<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({
    startOnLoad: true,
    theme: 'base',
    themeVariables: {
      // Primary colors from design system (Capcat orange palette)
      primaryColor: '#FFD4B7',           // --orange-200
      primaryTextColor: '#201419',       // --ink
      primaryBorderColor: '#F1540E',     // --orange-500 / --accent-primary

      // Line and edge colors
      lineColor: '#58444c',              // --ink-medium

      // Secondary colors
      secondaryColor: '#FFEADB',         // --orange-100
      tertiaryColor: '#f9f8ed',          // --accent-cream-primary

      // Text colors
      textColor: '#201419',              // --ink
      mainBkg: '#FAF8EE',                // --cream

      // Node styling
      nodeBorder: '#F1540E',             // --accent-primary
      clusterBkg: '#faf2e7',             // --accent-cream-light
      clusterBorder: '#D44400',          // --orange-600 / --accent-hover

      // Font
      fontFamily: '-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif',
      fontSize: '16px'
    },
    flowchart: {
      nodeSpacing: 50,
      rankSpacing: 50,
      padding: 15,
      useMaxWidth: true,
      htmlLabels: true,
      curve: 'basis'
    }
  });

  // Add copy buttons to Mermaid diagrams after rendering
  document.addEventListener('DOMContentLoaded', function() {
    const mermaidDivs = document.querySelectorAll('.mermaid');

    mermaidDivs.forEach(function(mermaidDiv) {
      // Get the original Mermaid source code
      const mermaidSource = mermaidDiv.textContent;

      // Create container wrapper
      const container = document.createElement('div');
      container.className = 'mermaid-container';

      // Create copy button
      const copyBtn = document.createElement('button');
      copyBtn.className = 'mermaid-copy-btn';
      copyBtn.textContent = 'Copy Mermaid Code';
      copyBtn.setAttribute('title', 'Copy diagram code for Draw.io, Mermaid Live, etc.');

      copyBtn.addEventListener('click', function() {
        navigator.clipboard.writeText(mermaidSource).then(function() {
          copyBtn.textContent = 'Copied!';
          copyBtn.classList.add('copied');

          setTimeout(function() {
            copyBtn.textContent = 'Copy Mermaid Code';
            copyBtn.classList.remove('copied');
          }, 2000);
        });
      });

      // Wrap the mermaid div in container and add button
      mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
      container.appendChild(mermaidDiv);
      container.appendChild(copyBtn);
    });
  });
</script>
    
    
    <script src="../js/main.js"></script>

            
<nav class="chapter-navigation" aria-label="Chapter navigation">
    <div class="next-chapter-content">
        <span class="next-chapter-label">Next Chapter</span>
        <span class="next-chapter-arrow" aria-hidden="true">→</span>
        <a href="architecture.html" class="next-chapter-link" rel="next">Architecture Overview</a>
    </div>
</nav>
</div>
        </div>
    </div>

    <!-- Footer -->
    {% include footer.html %}

    <!-- Back to Top Button -->
    <button id="backToTopBtn" title="Go to top">
        <svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" width="24">
            <path d="M0 0h24v24H0V0z" fill="none"/>
            <path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6 1.41 1.41z"/>
        </svg>
    </button>  </body>
