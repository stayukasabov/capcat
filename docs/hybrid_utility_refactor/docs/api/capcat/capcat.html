<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>capcat</title>
    <link rel="stylesheet" href="../../../css/design-system.css">
    <link rel="stylesheet" href="../../../css/main.css">
</head>
<body>
    <!-- Header -->
    <div id="header-placeholder"></div>


    <div class="doc-container">
        <div class="container">
            <div class="doc-content">

<div class="nav-breadcrumb"><a href="../../index.html">Documentation Home</a> / <a href="../index.html">api</a> / capcat</div>
<h1>capcat</h1>

<h4>File:</h4> <code>Application/capcat.py</code>

<h2>Description</h2>

<p>Capcat - News Article Archiving System</p>

<p>A free and open-source tool to make people&#x27;s lives easier.</p>

<p>Author: Stayu Kasabov (https://stayux.com)</p>
<p>License: MIT-Style Non-Commercial</p>
<p>Copyright (c) 2025 Stayu Kasabov</p>

<h2>Classes</h2>

<h3>GenericArticleFetcher</h3>

<h4>Inherits from:</h4> ArticleFetcher

<h3>Methods</h3>

<h5>should_skip_url</h5>

<pre><code>def should_skip_url(self, url: str, title: str) -&gt; bool
</code></pre>

<p>Never skip URLs for generic fetching.</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>url</code> (str)</li>
<li><code>title</code> (str)</li>
</ul>

<h4>Returns:</h4> bool

<h3>Args</h3>

<h3>Methods</h3>

<h5>__init__</h5>

<pre><code>def __init__(self, config_dict)
</code></pre>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>config_dict</code></li>
</ul>

<h3>Args</h3>

<h3>Methods</h3>

<h5>__init__</h5>

<pre><code>def __init__(self, config_dict)
</code></pre>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>config_dict</code></li>
</ul>

<h2>Functions</h2>

<h3>process_sources</h3>

<pre><code>def process_sources(sources: List[str], args: argparse.Namespace, config, logger, generate_html: bool = False, output_dir: str = &#x27;.&#x27;) -&gt; Dict[str, any]
</code></pre>

<p>Process multiple sources using the unified processor.</p>

<p>Args:</p>
<p>    sources: List of source identifiers to process (e.g., &#x27;hn&#x27;, &#x27;bbc&#x27;)</p>
<p>    args: Parsed command-line arguments containing count, quiet, verbose</p>
<p>    config: Configuration object with system settings</p>
<p>    logger: Logger instance for output</p>
<p>    generate_html: Whether to generate HTML output after processing</p>
<p>    output_dir: Output directory path for saved articles</p>

<p>Returns:</p>
<p>    Dictionary with keys:</p>
<ul>
<li>&#x27;successful&#x27;: List of successfully processed sources</li>
<li>&#x27;failed&#x27;: List of tuples (source, error_message)</li>
<li>&#x27;total&#x27;: Total number of sources attempted</li>
</ul>

<p>Raises:</p>
<p>    SourceError: If source cannot be loaded from registry</p>

<h4>Parameters:</h4>

<ul>
<li><code>sources</code> (List[str])</li>
<li><code>args</code> (argparse.Namespace)</li>
<li><code>config</code></li>
<li><code>logger</code></li>
<li><code>generate_html</code> (bool) <em>optional</em></li>
<li><code>output_dir</code> (str) <em>optional</em></li>
</ul>

<h4>Returns:</h4> Dict[str, any]

<h3>_scrape_with_specialized_source</h3>

<pre><code>def _scrape_with_specialized_source(url: str, output_dir: str, verbose: bool = False, files: bool = False, generate_html: bool = False) -&gt; Tuple[bool, Optional[str]]
</code></pre>

<p>Handle article scraping with specialized sources.</p>

<p>Supports Medium, Substack, and similar platforms with paywall detection.</p>

<p>Args:</p>
<p>    url: Article URL to scrape</p>
<p>    output_dir: Directory to save article content</p>
<p>    verbose: Enable verbose logging output</p>
<p>    files: Download all media files (PDFs, audio, video)</p>
<p>    generate_html: Generate HTML version of article</p>

<p>Returns:</p>
<p>    Tuple of (success status, output directory path). Returns</p>
<p>    (True, path) on success, (False, path) or (False, None) on failure.</p>

<p>Raises:</p>
<p>    SpecializedSourceError: If source detection or scraping fails</p>

<h4>Parameters:</h4>

<ul>
<li><code>url</code> (str)</li>
<li><code>output_dir</code> (str)</li>
<li><code>verbose</code> (bool) <em>optional</em></li>
<li><code>files</code> (bool) <em>optional</em></li>
<li><code>generate_html</code> (bool) <em>optional</em></li>
</ul>

<h4>Returns:</h4> Tuple[bool, Optional[str]]

<h3>scrape_single_article</h3>

<pre><code>def scrape_single_article(url: str, output_dir: str, verbose: bool = False, files: bool = False, generate_html: bool = False, update_mode: bool = False) -&gt; Tuple[bool, Optional[str]]
</code></pre>

<p>Scrape a single article from any supported source.</p>

<p>Attempts specialized sources first (Medium, Substack), then falls back</p>
<p>to generic scraping. Auto-detects source and creates organized output.</p>

<p>Args:</p>
<p>    url: Article URL to scrape</p>
<p>    output_dir: Base directory for output (uses ../Capcats/ if &quot;.&quot;)</p>
<p>    verbose: Enable verbose logging output</p>
<p>    files: Download all media files (PDFs, audio, video)</p>
<p>    generate_html: Generate HTML version of article</p>
<p>    update_mode: Update existing article instead of creating new</p>

<p>Returns:</p>
<p>    Tuple of (success status, output directory path). Returns</p>
<p>    (True, path) on success, (False, path) or (False, None) on failure.</p>

<p>Raises:</p>
<p>    SourceError: If source detection fails</p>
<p>    IOError: If output directory cannot be created</p>

<h4>Parameters:</h4>

<ul>
<li><code>url</code> (str)</li>
<li><code>output_dir</code> (str)</li>
<li><code>verbose</code> (bool) <em>optional</em></li>
<li><code>files</code> (bool) <em>optional</em></li>
<li><code>generate_html</code> (bool) <em>optional</em></li>
<li><code>update_mode</code> (bool) <em>optional</em></li>
</ul>

<h4>Returns:</h4> Tuple[bool, Optional[str]]

<p>WARNING: <h4>High complexity:</h4> 14</p>

<h3>run_app</h3>

<pre><code>def run_app(argv: Optional[List[str]] = None)
</code></pre>

<p>Main application logic.</p>

<h4>Parameters:</h4>

<ul>
<li><code>argv</code> (Optional[List[str]]) <em>optional</em></li>
</ul>

<p>WARNING: <h4>High complexity:</h4> 50</p>

<h3>main</h3>

<pre><code>def main() -&gt; None
</code></pre>

<p>Main entry point for the optimized Capcat application.</p>

<p>Parses command-line arguments and delegates to run_app for execution.</p>
<p>Handles single article scraping, batch processing, and interactive mode.</p>

<h4>Returns:</h4> None


<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({
    startOnLoad: true,
    theme: 'base',
    themeVariables: {
      // Primary colors from design system (Capcat orange palette)
      primaryColor: '#FFD4B7',           // --orange-200
      primaryTextColor: '#201419',       // --ink
      primaryBorderColor: '#F1540E',     // --orange-500 / --accent-primary

      // Line and edge colors
      lineColor: '#58444c',              // --ink-medium

      // Secondary colors
      secondaryColor: '#FFEADB',         // --orange-100
      tertiaryColor: '#f9f8ed',          // --accent-cream-primary

      // Text colors
      textColor: '#201419',              // --ink
      mainBkg: '#FAF8EE',                // --cream

      // Node styling
      nodeBorder: '#F1540E',             // --accent-primary
      clusterBkg: '#faf2e7',             // --accent-cream-light
      clusterBorder: '#D44400',          // --orange-600 / --accent-hover

      // Font
      fontFamily: '-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif',
      fontSize: '16px'
    },
    flowchart: {
      nodeSpacing: 50,
      rankSpacing: 50,
      padding: 15,
      useMaxWidth: true,
      htmlLabels: true,
      curve: 'basis'
    }
  });

  // Add copy buttons to Mermaid diagrams after rendering
  document.addEventListener('DOMContentLoaded', function() {
    const mermaidDivs = document.querySelectorAll('.mermaid');

    mermaidDivs.forEach(function(mermaidDiv) {
      // Get the original Mermaid source code
      const mermaidSource = mermaidDiv.textContent;

      // Create container wrapper
      const container = document.createElement('div');
      container.className = 'mermaid-container';

      // Create copy button
      const copyBtn = document.createElement('button');
      copyBtn.className = 'mermaid-copy-btn';
      copyBtn.textContent = 'Copy Mermaid Code';
      copyBtn.setAttribute('title', 'Copy diagram code for Draw.io, Mermaid Live, etc.');

      copyBtn.addEventListener('click', function() {
        navigator.clipboard.writeText(mermaidSource).then(function() {
          copyBtn.textContent = 'Copied!';
          copyBtn.classList.add('copied');

          setTimeout(function() {
            copyBtn.textContent = 'Copy Mermaid Code';
            copyBtn.classList.remove('copied');
          }, 2000);
        });
      });

      // Wrap the mermaid div in container and add button
      mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
      container.appendChild(mermaidDiv);
      container.appendChild(copyBtn);
    });
  });
</script>
    <script src="../../../js/main.js"></script>

            </div>
        </div>
    </div>

    <!-- Footer -->
    <div id="footer-placeholder"></div>

    <!-- Back to Top Button -->
    <button id="backToTopBtn" title="Go to top">
        <svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" width="24">
            <path d="M0 0h24v24H0V0z" fill="none"/>
            <path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6 1.41 1.41z"/>
        </svg>
    </button>

    <script src="../../../../js/includes-loader.js"></script>
  </body>
