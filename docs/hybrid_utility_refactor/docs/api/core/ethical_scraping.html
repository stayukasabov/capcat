<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>core.ethical_scraping</title>
    <link rel="stylesheet" href="../../../css/design-system.css">
    <link rel="stylesheet" href="../../../css/main.css">
</head>
<body>
    <!-- Header -->
    {% include header.html %}


    <div class="doc-container">
        <div class="container">
            <div class="doc-content">

<div class="nav-breadcrumb"><a href="../../index.html">Documentation Home</a> / <a href="../index.html">api</a> / core</div>
<h1>core.ethical_scraping</h1>

<h4>File:</h4> <code>Application/core/ethical_scraping.py</code>

<h2>Description</h2>

<p>Ethical scraping utilities for Capcat.</p>

<p>Implements best practices:</p>
<ol>
<li>Robots.txt caching with 15-minute TTL</li>
<li>429/503 error handling with exponential backoff</li>
<li>Rate limiting enforcement</li>
<li>Path validation against robots.txt</li>
</ol>

<h2>Classes</h2>

<h3>RobotsTxtCache</h3>

<p>Cache entry for robots.txt.</p>

<h3>EthicalScrapingManager</h3>

<p>Manages ethical scraping compliance.</p>

<p>Features:</p>
<ul>
<li>Robots.txt caching (15-minute TTL)</li>
<li>Crawl delay enforcement</li>
<li>429/503 exponential backoff</li>
<li>Path validation</li>
</ul>

<h3>Methods</h3>

<h5>__init__</h5>

<pre><code>def __init__(self, user_agent: str = &#x27;Capcat/2.0&#x27;)
</code></pre>

<p>Initialize ethical scraping manager.</p>

<p>Args:</p>
<p>    user_agent: User agent string for requests</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>user_agent</code> (str) <em>optional</em></li>
</ul>

<h5>get_robots_txt</h5>

<pre><code>def get_robots_txt(self, base_url: str, timeout: int = 10) -&gt; Tuple[RobotFileParser, float]
</code></pre>

<p>Fetch and parse robots.txt with caching.</p>

<p>Args:</p>
<p>    base_url: Base URL of the site</p>
<p>    timeout: Request timeout in seconds</p>

<p>Returns:</p>
<p>    Tuple of (RobotFileParser, crawl_delay)</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>base_url</code> (str)</li>
<li><code>timeout</code> (int) <em>optional</em></li>
</ul>

<h4>Returns:</h4> Tuple[RobotFileParser, float]

<h5>_extract_crawl_delay</h5>

<pre><code>def _extract_crawl_delay(self, parser: RobotFileParser) -&gt; float
</code></pre>

<p>Extract crawl delay from robots.txt parser.</p>

<p>Args:</p>
<p>    parser: RobotFileParser instance</p>

<p>Returns:</p>
<p>    Crawl delay in seconds (0.0 if not specified)</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>parser</code> (RobotFileParser)</li>
</ul>

<h4>Returns:</h4> float

<h5>can_fetch</h5>

<pre><code>def can_fetch(self, url: str) -&gt; Tuple[bool, str]
</code></pre>

<p>Check if URL can be fetched according to robots.txt.</p>

<p>Args:</p>
<p>    url: URL to check</p>

<p>Returns:</p>
<p>    Tuple of (allowed, reason)</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>url</code> (str)</li>
</ul>

<h4>Returns:</h4> Tuple[bool, str]

<h5>enforce_rate_limit</h5>

<pre><code>def enforce_rate_limit(self, domain: str, crawl_delay: float, min_delay: float = 1.0)
</code></pre>

<p>Enforce rate limiting with crawl delay.</p>

<p>Args:</p>
<p>    domain: Domain being accessed</p>
<p>    crawl_delay: Required crawl delay from robots.txt</p>
<p>    min_delay: Minimum delay even if robots.txt doesn&#x27;t specify</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>domain</code> (str)</li>
<li><code>crawl_delay</code> (float)</li>
<li><code>min_delay</code> (float) <em>optional</em></li>
</ul>

<h5>request_with_backoff</h5>

<pre><code>def request_with_backoff(self, session: requests.Session, url: str, method: str = &#x27;GET&#x27;, max_retries: int = 3, initial_delay: float = 1.0) -&gt; requests.Response
</code></pre>

<p>Make HTTP request with exponential backoff for 429/503 errors.</p>

<p>Args:</p>
<p>    session: Requests session</p>
<p>    url: URL to fetch</p>
<p>    method: HTTP method (GET, POST, etc.)</p>
<p>    max_retries: Maximum number of retries</p>
<p>    initial_delay: Initial retry delay in seconds</p>
<p>    <h4>kwargs: Additional arguments for requests</p>

<p>Returns:</p>
<p>    Response object</p>

<p>Raises:</p>
<p>    requests.RequestException: If all retries fail</p>

</strong>Parameters:*<em>

<ul>
<li><code>self</code></li>
<li><code>session</code> (requests.Session)</li>
<li><code>url</code> (str)</li>
<li><code>method</code> (str) </em>optional<em></li>
<li><code>max_retries</code> (int) </em>optional<em></li>
<li><code>initial_delay</code> (float) </em>optional<em></li>
</ul>

<h4>Returns:</h4> requests.Response

<p>WARNING: <h4>High complexity:</h4> 12</p>

<h5>validate_source_config</h5>

<pre><code>def validate_source_config(self, base_url: str, rate_limit: float) -&gt; Tuple[bool, str]
</code></pre>

<p>Validate source configuration against robots.txt.</p>

<p>Args:</p>
<p>    base_url: Base URL of the source</p>
<p>    rate_limit: Configured rate limit in seconds</p>

<p>Returns:</p>
<p>    Tuple of (valid, message)</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
<li><code>base_url</code> (str)</li>
<li><code>rate_limit</code> (float)</li>
</ul>

<h4>Returns:</h4> Tuple[bool, str]

<h5>get_cache_stats</h5>

<pre><code>def get_cache_stats(self) -&gt; Dict[str, any]
</code></pre>

<p>Get statistics about robots.txt cache.</p>

<p>Returns:</p>
<p>    Dictionary with cache statistics</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
</ul>

<h4>Returns:</h4> Dict[str, any]

<h5>clear_stale_cache</h5>

<pre><code>def clear_stale_cache(self)
</code></pre>

<p>Remove stale entries from robots.txt cache.</p>

<h4>Parameters:</h4>

<ul>
<li><code>self</code></li>
</ul>

<h2>Functions</h2>

<h3>get_ethical_manager</h3>

<pre><code>def get_ethical_manager(user_agent: str = &#x27;Capcat/2.0&#x27;) -&gt; EthicalScrapingManager
</code></pre>

<p>Get or create global ethical scraping manager.</p>

<p>Args:</p>
<p>    user_agent: User agent string</p>

<p>Returns:</p>
<p>    EthicalScrapingManager instance</p>

<h4>Parameters:</h4>

<ul>
<li><code>user_agent</code> (str) </em>optional*</li>
</ul>

<h4>Returns:</h4> EthicalScrapingManager


<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({
    startOnLoad: true,
    theme: 'base',
    themeVariables: {
      // Primary colors from design system (Capcat orange palette)
      primaryColor: '#FFD4B7',           // --orange-200
      primaryTextColor: '#201419',       // --ink
      primaryBorderColor: '#F1540E',     // --orange-500 / --accent-primary

      // Line and edge colors
      lineColor: '#58444c',              // --ink-medium

      // Secondary colors
      secondaryColor: '#FFEADB',         // --orange-100
      tertiaryColor: '#f9f8ed',          // --accent-cream-primary

      // Text colors
      textColor: '#201419',              // --ink
      mainBkg: '#FAF8EE',                // --cream

      // Node styling
      nodeBorder: '#F1540E',             // --accent-primary
      clusterBkg: '#faf2e7',             // --accent-cream-light
      clusterBorder: '#D44400',          // --orange-600 / --accent-hover

      // Font
      fontFamily: '-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif',
      fontSize: '16px'
    },
    flowchart: {
      nodeSpacing: 50,
      rankSpacing: 50,
      padding: 15,
      useMaxWidth: true,
      htmlLabels: true,
      curve: 'basis'
    }
  });

  // Add copy buttons to Mermaid diagrams after rendering
  document.addEventListener('DOMContentLoaded', function() {
    const mermaidDivs = document.querySelectorAll('.mermaid');

    mermaidDivs.forEach(function(mermaidDiv) {
      // Get the original Mermaid source code
      const mermaidSource = mermaidDiv.textContent;

      // Create container wrapper
      const container = document.createElement('div');
      container.className = 'mermaid-container';

      // Create copy button
      const copyBtn = document.createElement('button');
      copyBtn.className = 'mermaid-copy-btn';
      copyBtn.textContent = 'Copy Mermaid Code';
      copyBtn.setAttribute('title', 'Copy diagram code for Draw.io, Mermaid Live, etc.');

      copyBtn.addEventListener('click', function() {
        navigator.clipboard.writeText(mermaidSource).then(function() {
          copyBtn.textContent = 'Copied!';
          copyBtn.classList.add('copied');

          setTimeout(function() {
            copyBtn.textContent = 'Copy Mermaid Code';
            copyBtn.classList.remove('copied');
          }, 2000);
        });
      });

      // Wrap the mermaid div in container and add button
      mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
      container.appendChild(mermaidDiv);
      container.appendChild(copyBtn);
    });
  });
</script>
    <script src="../../../js/main.js"></script>

            </div>
        </div>
    </div>

    <!-- Footer -->
    {% include footer.html %}

    <!-- Back to Top Button -->
    <button id="backToTopBtn" title="Go to top">
        <svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" width="24">
            <path d="M0 0h24v24H0V0z" fill="none"/>
            <path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6 1.41 1.41z"/>
        </svg>
    </button>  </body>
