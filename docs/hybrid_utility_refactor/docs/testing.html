<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Testing Guide</title>
    <link rel="stylesheet" href="../css/design-system.css">
    <link rel="stylesheet" href="../css/main.css">
</head>
<body>
    <!-- Header -->
    <div id="header-placeholder"></div>


    <div class="doc-container">
        <div class="container">
            <div class="doc-content">

<div class="nav-breadcrumb"><a href="index.html">Documentation Home</a></div>
<h1>Testing Guide</h1>

<p>Comprehensive testing procedures and standards for Capcat&#x27;s hybrid architecture.</p>

<h2>Testing Philosophy</h2>

<p>Capcat follows <h4>systematic testing standards</h4> with mandatory procedures for all testing tasks:</p>

<ol>
<li><h4>Complete Coverage</h4> Test ALL supported sources, not random subsets</li>
<li><h4>Mandatory Documentation</h4> Create test-diagnose files for EVERY test</li>
<li><h4>Systematic Analysis</h4> Read all results and create prioritized fix plans</li>
<li><h4>Universal Success Criteria</h4> Different criteria for sources vs components vs features</li>
</ol>

<h2>Testing Types</h2>

<h3>1. Source Testing</h3>
<h4>Purpose</h4> Validate individual source functionality
<h4>Scope</h4> All 20 production sources
<h4>Frequency</h4> Before deployment, after changes

<h3>2. Component Testing</h3>
<h4>Purpose</h4> Validate core system components
<h4>Scope</h4> Registry, factory, monitor, validation engine
<h4>Frequency</h4> After component changes

<h3>3. Integration Testing</h3>
<h4>Purpose</h4> Validate end-to-end workflows
<h4>Scope</h4> Full system functionality
<h4>Frequency</h4> Before major releases

<h3>4. Performance Testing</h3>
<h4>Purpose</h4> Validate system performance characteristics
<h4>Scope</h4> Speed, memory usage, network efficiency
<h4>Frequency</h4> Regular monitoring

<h2>Source Testing Procedures</h2>

<h3>Mandatory Source Testing Protocol</h3>

<h4>MANDATORY</h4> For ALL source testing tasks, follow this systematic procedure:

<h3>1. Pre-Test Setup</h3>
<pre><code># Activate environment
source venv/bin/activate

# Verify source discovery
python -c &quot;from core.source_system.source_registry import get_source_registry; print(f&#x27;{len(get_source_registry().get_available_sources())} sources discovered&#x27;)&quot;
</code></pre>

<h3>2. Individual Source Testing</h3>
<p>Test each source individually with diagnosis file creation:</p>

<pre><code># Test each source with output capture
./capcat fetch hn --count 10 2&gt;&amp;1 | tee test-diagnose-hn.txt
./capcat fetch lb --count 10 2&gt;&amp;1 | tee test-diagnose-lb.txt
./capcat fetch iq --count 10 2&gt;&amp;1 | tee test-diagnose-iq.txt
./capcat fetch bbc --count 10 2&gt;&amp;1 | tee test-diagnose-bbc.txt
./capcat fetch cnn --count 10 2&gt;&amp;1 | tee test-diagnose-cnn.txt
./capcat fetch techcrunch --count 10 2&gt;&amp;1 | tee test-diagnose-techcrunch.txt
./capcat fetch theverge --count 10 2&gt;&amp;1 | tee test-diagnose-theverge.txt
./capcat fetch wired --count 10 2&gt;&amp;1 | tee test-diagnose-wired.txt
./capcat fetch nature --count 10 2&gt;&amp;1 | tee test-diagnose-nature.txt
./capcat fetch scientificamerican --count 10 2&gt;&amp;1 | tee test-diagnose-scientificamerican.txt
./capcat fetch mittechreview --count 10 2&gt;&amp;1 | tee test-diagnose-mittechreview.txt
./capcat fetch gizmodo --count 10 2&gt;&amp;1 | tee test-diagnose-gizmodo.txt
./capcat fetch upi --count 10 2&gt;&amp;1 | tee test-diagnose-upi.txt
./capcat fetch tass --count 10 2&gt;&amp;1 | tee test-diagnose-tass.txt
./capcat fetch straitstimes --count 10 2&gt;&amp;1 | tee test-diagnose-straitstimes.txt
./capcat fetch euronews --count 10 2&gt;&amp;1 | tee test-diagnose-euronews.txt
./capcat fetch axios --count 10 2&gt;&amp;1 | tee test-diagnose-axios.txt
./capcat fetch yahoo --count 10 2&gt;&amp;1 | tee test-diagnose-yahoo.txt
./capcat fetch xinhua --count 10 2&gt;&amp;1 | tee test-diagnose-xinhua.txt
./capcat fetch test_source --count 10 2&gt;&amp;1 | tee test-diagnose-test_source.txt
</code></pre>

<h3>3. Media Filtering Verification</h3>
<p>After each test, verify media filtering works correctly:</p>

<pre><code># Count non-image media files (should be 0 without --media flag)
find &quot;../News/news_*&quot; -name &quot;*.pdf&quot; -o -name &quot;*.mp4&quot; -o -name &quot;*.mp3&quot; | wc -l

# Count image files (should be &gt; 0)
find &quot;../News/news_*&quot; -name &quot;*.jpg&quot; -o -name &quot;*.png&quot; | wc -l
</code></pre>

<h3>4. Test-Diagnose File Creation</h3>

<h4>MANDATORY</h4> Create standardized diagnosis file for each source:

<pre><code># test-diagnose-[SOURCE_NAME].md

**Date**: YYYY-MM-DD HH:MM
**Test Type**: SOURCE_TEST
**Command**: ./capcat fetch [source] --count 10
**Status**: [SUCCESS/FAILURE/PARTIAL/ERROR]

## Results Summary
- **Items Requested**: 10
- **Items Successfully Processed**: X/10
- **Success Rate**: X%
- **Media Files Downloaded**: X images
- **Non-Image Media Files**: 0 (should be 0 unless --media used)

## Core Functionality Verification
- +/- Primary function works without errors
- +/- Media filtering works correctly
- +/- Output format/structure is correct
- +/- Error handling works properly
- +/- Performance is acceptable

## Errors Encountered
[List any errors: import issues, network failures, logic errors, etc.]

## Output Structure/Results
</code></pre>
<p>[Directory structure and file contents]</p>
<pre><code>
## Recommendations
[Specific fixes needed: import statements, logic changes, performance improvements]

## Priority Level
[HIGH/MEDIUM/LOW] - Based on impact on core functionality
</code></pre>

<h3>Comprehensive Source Testing Script</h3>

<pre><code>#!/usr/bin/env python3
&quot;&quot;&quot;
Comprehensive source testing script.
Tests all 20 sources with performance monitoring.
&quot;&quot;&quot;

import time
import subprocess
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
from core.source_system.source_registry import get_source_registry
from core.source_system.performance_monitor import get_performance_monitor

class SourceTester:
    def __init__(self):
        self.registry = get_source_registry()
        self.monitor = get_performance_monitor()
        self.results = {}

    def test_source(self, source_name: str, count: int = 10) -&gt; dict:
        &quot;&quot;&quot;Test a single source.&quot;&quot;&quot;
        print(f&quot;Testing {source_name}...&quot;)
        start_time = time.time()

        try:
            # Test source creation
            source = self.registry.get_source(source_name)
            creation_success = True
        except Exception as e:
            return {
                &#x27;source_name&#x27;: source_name,
                &#x27;creation_success&#x27;: False,
                &#x27;creation_error&#x27;: str(e),
                &#x27;articles_discovered&#x27;: 0,
                &#x27;processing_time&#x27;: 0
            }

        try:
            # Test article discovery
            articles = source.get_articles(count)
            articles_discovered = len(articles)
            discovery_success = articles_discovered &gt; 0
        except Exception as e:
            return {
                &#x27;source_name&#x27;: source_name,
                &#x27;creation_success&#x27;: True,
                &#x27;discovery_success&#x27;: False,
                &#x27;discovery_error&#x27;: str(e),
                &#x27;articles_discovered&#x27;: 0,
                &#x27;processing_time&#x27;: time.time() - start_time
            }

        processing_time = time.time() - start_time

        return {
            &#x27;source_name&#x27;: source_name,
            &#x27;creation_success&#x27;: creation_success,
            &#x27;discovery_success&#x27;: discovery_success,
            &#x27;articles_discovered&#x27;: articles_discovered,
            &#x27;processing_time&#x27;: processing_time,
            &#x27;avg_time_per_article&#x27;: processing_time / max(articles_discovered, 1)
        }

    def test_all_sources(self, count: int = 10) -&gt; dict:
        &quot;&quot;&quot;Test all available sources.&quot;&quot;&quot;
        sources = self.registry.get_available_sources()
        print(f&quot;Testing {len(sources)} sources...&quot;)

        # Parallel testing
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(self.test_source, source, count): source
                for source in sources
            }

            for future in futures:
                source_name = futures[future]
                try:
                    result = future.result()
                    self.results[source_name] = result
                except Exception as e:
                    self.results[source_name] = {
                        &#x27;source_name&#x27;: source_name,
                        &#x27;creation_success&#x27;: False,
                        &#x27;error&#x27;: str(e),
                        &#x27;processing_time&#x27;: 0
                    }

        return self.results

    def generate_report(self) -&gt; str:
        &quot;&quot;&quot;Generate comprehensive test report.&quot;&quot;&quot;
        total_sources = len(self.results)
        successful_creation = sum(1 for r in self.results.values() if r.get(&#x27;creation_success&#x27;, False))
        successful_discovery = sum(1 for r in self.results.values() if r.get(&#x27;discovery_success&#x27;, False))

        report = f&quot;&quot;&quot;# Comprehensive Source Test Report

**Date**: {time.strftime(&#x27;%Y-%m-%d %H:%M:%S&#x27;)}
**Sources Tested**: {total_sources}
**Creation Success Rate**: {successful_creation}/{total_sources} ({successful_creation/total_sources*100:.1f}%)
**Discovery Success Rate**: {successful_discovery}/{total_sources} ({successful_discovery/total_sources*100:.1f}%)

## Individual Source Results

&quot;&quot;&quot;

        for source_name, result in sorted(self.results.items()):
            status = &quot;SUCCESS&quot; if result.get(&#x27;discovery_success&#x27;, False) else &quot;FAILED&quot;
            articles = result.get(&#x27;articles_discovered&#x27;, 0)
            time_taken = result.get(&#x27;processing_time&#x27;, 0)

            report += f&quot;### {source_name}\n&quot;
            report += f&quot;- **Status**: {status}\n&quot;
            report += f&quot;- **Articles Discovered**: {articles}\n&quot;
            report += f&quot;- **Processing Time**: {time_taken:.2f}s\n&quot;

            if &#x27;error&#x27; in result or &#x27;creation_error&#x27; in result or &#x27;discovery_error&#x27; in result:
                error = result.get(&#x27;error&#x27;) or result.get(&#x27;creation_error&#x27;) or result.get(&#x27;discovery_error&#x27;)
                report += f&quot;- **Error**: {error}\n&quot;

            report += &quot;\n&quot;

        # Performance summary
        avg_time = sum(r.get(&#x27;processing_time&#x27;, 0) for r in self.results.values()) / total_sources
        total_articles = sum(r.get(&#x27;articles_discovered&#x27;, 0) for r in self.results.values())

        report += f&quot;&quot;&quot;## Performance Summary

- **Average Processing Time**: {avg_time:.2f}s per source
- **Total Articles Discovered**: {total_articles}
- **Overall Success Rate**: {successful_discovery/total_sources*100:.1f}%

## Recommendations

&quot;&quot;&quot;

        # Add recommendations based on results
        failed_sources = [name for name, result in self.results.items()
                         if not result.get(&#x27;discovery_success&#x27;, False)]

        if failed_sources:
            report += f&quot;**Failed Sources**: {&#x27;, &#x27;.join(failed_sources)}\n&quot;
            report += &quot;- Review configuration and network connectivity\n&quot;
            report += &quot;- Check for anti-bot protection\n&quot;
            report += &quot;- Validate CSS selectors\n\n&quot;

        slow_sources = [name for name, result in self.results.items()
                       if result.get(&#x27;processing_time&#x27;, 0) &gt; 10]

        if slow_sources:
            report += f&quot;**Slow Sources**: {&#x27;, &#x27;.join(slow_sources)}\n&quot;
            report += &quot;- Consider optimization\n&quot;
            report += &quot;- Review timeout settings\n&quot;

        return report

if __name__ == &quot;__main__&quot;:
    tester = SourceTester()
    results = tester.test_all_sources(count=10)

    report = tester.generate_report()

    # Save report
    with open(&#x27;comprehensive_test_report.md&#x27;, &#x27;w&#x27;) as f:
        f.write(report)

    print(&quot;Test completed. Report saved to comprehensive_test_report.md&quot;)
    print(f&quot;Success rate: {sum(1 for r in results.values() if r.get(&#x27;discovery_success&#x27;, False))}/{len(results)}&quot;)
</code></pre>

<h2>Component Testing</h2>

<h3>Registry Testing</h3>
<pre><code># test_registry.py
import unittest
from core.source_system.source_registry import SourceRegistry, get_source_registry

class TestSourceRegistry(unittest.TestCase):
    def setUp(self):
        self.registry = SourceRegistry()

    def test_source_discovery(self):
        &quot;&quot;&quot;Test source discovery functionality.&quot;&quot;&quot;
        sources = self.registry.discover_sources()
        self.assertGreater(len(sources), 0)
        self.assertIn(&#x27;hn&#x27;, sources)
        self.assertIn(&#x27;bbc&#x27;, sources)

    def test_source_creation(self):
        &quot;&quot;&quot;Test source instance creation.&quot;&quot;&quot;
        source = self.registry.get_source(&#x27;hn&#x27;)
        self.assertIsNotNone(source)
        self.assertEqual(source.config.name, &#x27;hn&#x27;)

    def test_validation(self):
        &quot;&quot;&quot;Test source validation.&quot;&quot;&quot;
        errors = self.registry.validate_all_sources()
        # Should be minimal errors in production system
        error_count = sum(len(errs) for errs in errors.values())
        self.assertLessEqual(error_count, 5)  # Allow some minor issues

if __name__ == &#x27;__main__&#x27;:
    unittest.main()
</code></pre>

<h3>Performance Monitor Testing</h3>
<pre><code># test_performance_monitor.py
import unittest
import time
from core.source_system.performance_monitor import PerformanceMonitor

class TestPerformanceMonitor(unittest.TestCase):
    def setUp(self):
        self.monitor = PerformanceMonitor()

    def test_metrics_recording(self):
        &quot;&quot;&quot;Test metrics recording.&quot;&quot;&quot;
        self.monitor.record_request(&#x27;test_source&#x27;, True, 1.5)
        self.monitor.record_request(&#x27;test_source&#x27;, False, 2.0)

        metrics = self.monitor.get_source_metrics(&#x27;test_source&#x27;)
        self.assertEqual(metrics.total_requests, 2)
        self.assertEqual(metrics.successful_requests, 1)
        self.assertEqual(metrics.success_rate, 50.0)

    def test_performance_report(self):
        &quot;&quot;&quot;Test report generation.&quot;&quot;&quot;
        self.monitor.record_request(&#x27;test&#x27;, True, 1.0)
        report = self.monitor.generate_performance_report()
        self.assertIn(&#x27;test&#x27;, report)
        self.assertIn(&#x27;100.0%&#x27;, report)

if __name__ == &#x27;__main__&#x27;:
    unittest.main()
</code></pre>

<h2>Integration Testing</h2>

<h3>End-to-End Workflow Test</h3>
<pre><code># test_e2e_workflow.py
import unittest
import tempfile
import shutil
from pathlib import Path
from core.source_system.source_registry import get_source_registry

class TestE2EWorkflow(unittest.TestCase):
    def setUp(self):
        self.test_dir = Path(tempfile.mkdtemp())
        self.registry = get_source_registry()

    def tearDown(self):
        shutil.rmtree(self.test_dir)

    def test_article_fetch_workflow(self):
        &quot;&quot;&quot;Test complete article fetching workflow.&quot;&quot;&quot;
        # Get source
        source = self.registry.get_source(&#x27;hn&#x27;)

        # Fetch articles
        articles = source.get_articles(count=3)
        self.assertGreater(len(articles), 0)

        # Fetch content for first article
        if articles:
            content = source.get_article_content(articles[0][&#x27;url&#x27;])
            self.assertIsNotNone(content)

    def test_config_driven_vs_custom(self):
        &quot;&quot;&quot;Test both source types work.&quot;&quot;&quot;
        # Config-driven source
        config_source = self.registry.get_source(&#x27;iq&#x27;)
        config_articles = config_source.get_articles(count=3)

        # Custom source
        custom_source = self.registry.get_source(&#x27;hn&#x27;)
        custom_articles = custom_source.get_articles(count=3)

        self.assertGreater(len(config_articles), 0)
        self.assertGreater(len(custom_articles), 0)

if __name__ == &#x27;__main__&#x27;:
    unittest.main()
</code></pre>

<h2>Performance Testing</h2>

<h3>Performance Benchmarking Script</h3>
<pre><code># benchmark_performance.py
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
from core.source_system.source_registry import get_source_registry

class PerformanceBenchmark:
    def __init__(self):
        self.registry = get_source_registry()

    def benchmark_source(self, source_name: str, iterations: int = 5) -&gt; dict:
        &quot;&quot;&quot;Benchmark a single source.&quot;&quot;&quot;
        times = []
        article_counts = []

        for _ in range(iterations):
            start_time = time.time()

            source = self.registry.get_source(source_name)
            articles = source.get_articles(count=10)

            duration = time.time() - start_time
            times.append(duration)
            article_counts.append(len(articles))

        return {
            &#x27;source_name&#x27;: source_name,
            &#x27;avg_time&#x27;: statistics.mean(times),
            &#x27;min_time&#x27;: min(times),
            &#x27;max_time&#x27;: max(times),
            &#x27;std_dev&#x27;: statistics.stdev(times) if len(times) &gt; 1 else 0,
            &#x27;avg_articles&#x27;: statistics.mean(article_counts),
            &#x27;consistency&#x27;: statistics.stdev(article_counts) if len(article_counts) &gt; 1 else 0
        }

    def benchmark_all_sources(self, iterations: int = 3) -&gt; dict:
        &quot;&quot;&quot;Benchmark all sources.&quot;&quot;&quot;
        sources = self.registry.get_available_sources()[:5]  # Test subset for speed
        results = {}

        for source in sources:
            try:
                results[source] = self.benchmark_source(source, iterations)
            except Exception as e:
                results[source] = {&#x27;error&#x27;: str(e)}

        return results

    def generate_performance_report(self, results: dict) -&gt; str:
        &quot;&quot;&quot;Generate performance report.&quot;&quot;&quot;
        report = &quot;# Performance Benchmark Report\n\n&quot;

        successful_results = {k: v for k, v in results.items() if &#x27;error&#x27; not in v}

        if successful_results:
            avg_times = [r[&#x27;avg_time&#x27;] for r in successful_results.values()]
            overall_avg = statistics.mean(avg_times)

            report += f&quot;**Overall Average Time**: {overall_avg:.2f}s\n&quot;
            report += f&quot;**Sources Tested**: {len(successful_results)}\n\n&quot;

            report += &quot;## Individual Source Performance\n\n&quot;

            for source, data in sorted(successful_results.items(), key=lambda x: x[1][&#x27;avg_time&#x27;]):
                report += f&quot;### {source}\n&quot;
                report += f&quot;- Average Time: {data[&#x27;avg_time&#x27;]:.2f}s\n&quot;
                report += f&quot;- Range: {data[&#x27;min_time&#x27;]:.2f}s - {data[&#x27;max_time&#x27;]:.2f}s\n&quot;
                report += f&quot;- Articles Found: {data[&#x27;avg_articles&#x27;]:.1f} avg\n&quot;
                report += f&quot;- Consistency: {data[&#x27;std_dev&#x27;]:.2f}s std dev\n\n&quot;

        # Failed sources
        failed_sources = {k: v for k, v in results.items() if &#x27;error&#x27; in v}
        if failed_sources:
            report += &quot;## Failed Sources\n\n&quot;
            for source, data in failed_sources.items():
                report += f&quot;- **{source}**: {data[&#x27;error&#x27;]}\n&quot;

        return report

if __name__ == &quot;__main__&quot;:
    benchmark = PerformanceBenchmark()
    results = benchmark.benchmark_all_sources(iterations=3)

    report = benchmark.generate_performance_report(results)

    with open(&#x27;performance_benchmark.md&#x27;, &#x27;w&#x27;) as f:
        f.write(report)

    print(&quot;Performance benchmark completed. Report saved to performance_benchmark.md&quot;)
</code></pre>

<h2>Success Criteria</h2>

<h3>Source Testing Success Criteria</h3>
<ul>
<li>Command executes without import/syntax errors</li>
<li>Successfully processes at least 80% of requested articles</li>
<li>Media filtering works correctly (only images without --media flag)</li>
<li>Proper directory structure created</li>
<li>Articles saved in readable Markdown format</li>
</ul>

<h3>Component Testing Success Criteria</h3>
<ul>
<li>Component initializes without errors</li>
<li>Core functionality works as designed</li>
<li>Error handling works properly</li>
<li>Performance meets acceptable standards</li>
<li>Integration with other components works</li>
</ul>

<h3>Performance Testing Success Criteria</h3>
<ul>
<li>Average response time &lt; 6 seconds per source</li>
<li>Success rate &gt; 85% across all sources</li>
<li>Memory usage remains stable during batch operations</li>
<li>No memory leaks during extended operation</li>
</ul>

<h2>Debugging and Troubleshooting</h2>

<h3>Enable Debug Logging</h3>
<pre><code>import logging
logging.basicConfig(level=logging.DEBUG)
</code></pre>

<h3>Common Issues and Solutions</h3>

<h3>Import Errors</h3>
<pre><code># Check Python path
python -c &quot;import sys; print(&#x27;\n&#x27;.join(sys.path))&quot;

# Verify virtual environment
which python
pip list | grep -E &quot;(requests|beautifulsoup4|yaml)&quot;
</code></pre>

<h3>Network Issues</h3>
<pre><code># Test network connectivity
import requests
response = requests.get(&#x27;https://httpbin.org/get&#x27;, timeout=5)
print(f&quot;Status: {response.status_code}&quot;)
</code></pre>

<h3>Source Discovery Issues</h3>
<pre><code># Debug source discovery
from core.source_system.source_registry import SourceRegistry
registry = SourceRegistry()
registry.logger.setLevel(logging.DEBUG)
sources = registry.discover_sources()
</code></pre>

<h3>Test Data Validation</h3>
<pre><code># Validate test output structure
import os
from pathlib import Path

def validate_test_output(base_path=&quot;../News&quot;):
    &quot;&quot;&quot;Validate test output structure.&quot;&quot;&quot;
    news_dirs = list(Path(base_path).glob(&quot;news_*&quot;))

    if not news_dirs:
        print(&quot;No news directories found&quot;)
        return False

    latest_dir = max(news_dirs, key=os.path.getctime)
    print(f&quot;Found news directory: {latest_dir}&quot;)

    source_dirs = list(latest_dir.glob(&quot;*_*&quot;))
    print(f&quot;Source directories: {len(source_dirs)}&quot;)

    for source_dir in source_dirs:
        article_dirs = list(source_dir.glob(&quot;*_*&quot;))
        print(f&quot;  - {source_dir.name}: {len(article_dirs)} articles&quot;)

        if article_dirs:
            sample_article = article_dirs[0]
            article_md = sample_article / &quot;article.md&quot;
            if article_md.exists():
                print(f&quot;    {sample_article.name}/article.md exists&quot;)
            else:
                print(f&quot;    {sample_article.name}/article.md missing&quot;)

if __name__ == &quot;__main__&quot;:
    validate_test_output()
</code></pre>



<em>This testing guide ensures comprehensive validation of all Capcat components with systematic procedures and clear success criteria.</em>

<script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({
    startOnLoad: true,
    theme: 'base',
    themeVariables: {
      // Primary colors from design system (Capcat orange palette)
      primaryColor: '#FFD4B7',           // --orange-200
      primaryTextColor: '#201419',       // --ink
      primaryBorderColor: '#F1540E',     // --orange-500 / --accent-primary

      // Line and edge colors
      lineColor: '#58444c',              // --ink-medium

      // Secondary colors
      secondaryColor: '#FFEADB',         // --orange-100
      tertiaryColor: '#f9f8ed',          // --accent-cream-primary

      // Text colors
      textColor: '#201419',              // --ink
      mainBkg: '#FAF8EE',                // --cream

      // Node styling
      nodeBorder: '#F1540E',             // --accent-primary
      clusterBkg: '#faf2e7',             // --accent-cream-light
      clusterBorder: '#D44400',          // --orange-600 / --accent-hover

      // Font
      fontFamily: '-apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif',
      fontSize: '16px'
    },
    flowchart: {
      nodeSpacing: 50,
      rankSpacing: 50,
      padding: 15,
      useMaxWidth: true,
      htmlLabels: true,
      curve: 'basis'
    }
  });

  // Add copy buttons to Mermaid diagrams after rendering
  document.addEventListener('DOMContentLoaded', function() {
    const mermaidDivs = document.querySelectorAll('.mermaid');

    mermaidDivs.forEach(function(mermaidDiv) {
      // Get the original Mermaid source code
      const mermaidSource = mermaidDiv.textContent;

      // Create container wrapper
      const container = document.createElement('div');
      container.className = 'mermaid-container';

      // Create copy button
      const copyBtn = document.createElement('button');
      copyBtn.className = 'mermaid-copy-btn';
      copyBtn.textContent = 'Copy Mermaid Code';
      copyBtn.setAttribute('title', 'Copy diagram code for Draw.io, Mermaid Live, etc.');

      copyBtn.addEventListener('click', function() {
        navigator.clipboard.writeText(mermaidSource).then(function() {
          copyBtn.textContent = 'Copied!';
          copyBtn.classList.add('copied');

          setTimeout(function() {
            copyBtn.textContent = 'Copy Mermaid Code';
            copyBtn.classList.remove('copied');
          }, 2000);
        });
      });

      // Wrap the mermaid div in container and add button
      mermaidDiv.parentNode.insertBefore(container, mermaidDiv);
      container.appendChild(mermaidDiv);
      container.appendChild(copyBtn);
    });
  });
</script>
    <script src="../js/main.js"></script>

            
<nav class="chapter-navigation" aria-label="Chapter navigation">
    <div class="next-chapter-content">
        <span class="next-chapter-label">Next Chapter</span>
        <span class="next-chapter-arrow" aria-hidden="true">â†’</span>
        <a href="deployment.html" class="next-chapter-link" rel="next">Deployment Guide</a>
    </div>
</nav>
</div>
        </div>
    </div>

    <!-- Footer -->
    <div id="footer-placeholder"></div>

    <!-- Back to Top Button -->
    <button id="backToTopBtn" title="Go to top">
        <svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" width="24">
            <path d="M0 0h24v24H0V0z" fill="none"/>
            <path fill="currentColor" d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6 1.41 1.41z"/>
        </svg>
    </button>

    <script src="../../js/includes-loader.js"></script>
  </body>
